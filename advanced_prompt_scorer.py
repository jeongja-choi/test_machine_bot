import streamlit as st

# í˜ì´ì§€ ì„¤ì • (ë°˜ë“œì‹œ ì²« ë²ˆì§¸ Streamlit ëª…ë ¹ì–´ì—¬ì•¼ í•¨)
st.set_page_config(
    page_title="Advanced GPT-4.0 Prompt Scorer",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

import pandas as pd
import numpy as np
import re
from io import StringIO
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# ì‚¬ìš©ì ì •ì˜ CSS
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        color: white;
        text-align: center;
    }
    .evidence-box {
        background: #1a1a2e;
        color: #ffffff;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #2196F3;
        margin: 1rem 0;
    }
    .temperature-warning {
        background: #2d2d44;
        color: #ffffff;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #ffc107;
        margin: 1rem 0;
    }
    .improvement-suggestion {
        background: #1e3a2e;
        color: #ffffff;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #28a745;
        margin: 0.5rem 0;
    }
    .sample-info {
        background: #2a2a3e;
        color: #ffffff;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #17a2b8;
        margin: 1rem 0;
    }
    .result-box {
        background: #3a2a4e;
        color: #ffffff;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #6f42c1;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ë©”ì¸ í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ¯ ê³ ê¸‰ GPT-4.0-mini ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìŠ¤ì½”ì–´ ì±„ì ê¸°</h1>
    <p>ê·¼ê±° ê¸°ë°˜ ë¶„ì„ê³¼ ì˜¨ë„ ì„¤ì •ì„ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìµœì í™” ë„êµ¬</p>
</div>
""", unsafe_allow_html=True)

class AdvancedPromptScorer:
    def __init__(self):
        self.scoring_criteria = {
            'accuracy': 0.90,
            'length': 0.10
        }
        self.max_length = 3000
        self.optimal_temperature = 0.4  # ì˜¨ë„ ì„¤ì • 40
        # ì˜¨ë„ 40ì— ìµœì í™”ëœ ë¼ë²¨ ì„ê³„ê°’ (ë” ì—„ê²©í•œ ê¸°ì¤€)
        self.label_threshold = 75  # ì˜¨ë„ 40ì—ì„œ ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë†’ì€ ì„ê³„ê°’
        
        # ê·¼ê±° ê¸°ë°˜ ë¶„ì„ì„ ìœ„í•œ ì°¸ì¡° ë°ì´í„°
        self.evidence_base = {
            'role_definition': {
                'importance': 95,
                'evidence': "OpenAI ì—°êµ¬ì— ë”°ë¥´ë©´ ëª…í™•í•œ ì—­í•  ì •ì˜ëŠ” ì‘ë‹µ í’ˆì§ˆì„ 95% í–¥ìƒì‹œí‚´",
                'examples': ["ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤", "ë‹¹ì‹ ì€ ê²½í—˜ì´ í’ë¶€í•œ ë§ˆì¼€íŒ… ì „ë¬¸ê°€ë¡œì„œ"]
            },
            'step_by_step': {
                'importance': 88,
                'evidence': "Chain-of-Thought ì—°êµ¬ ê²°ê³¼, ë‹¨ê³„ë³„ ì§€ì‹œëŠ” ì •í™•ë„ë¥¼ 88% í–¥ìƒ",
                'examples': ["ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰í•˜ì„¸ìš”", "1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘, 2ë‹¨ê³„: ë¶„ì„"]
            },
            'examples_inclusion': {
                'importance': 82,
                'evidence': "Few-shot learning ì—°êµ¬ì—ì„œ ì˜ˆì‹œ í¬í•¨ ì‹œ ì„±ëŠ¥ 82% ê°œì„  í™•ì¸",
                'examples': ["ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•˜ì„¸ìš”", "êµ¬ì²´ì ì¸ ì˜ˆì‹œ: [ìƒ˜í”Œ ë°ì´í„°]"]
            },
            'constraint_specification': {
                'importance': 76,
                'evidence': "ì œì•½ ì¡°ê±´ ëª…ì‹œëŠ” ê³¼ì í•© ë°©ì§€ ë° ì •í™•ì„± 76% í–¥ìƒ",
                'examples': ["ë‹¨, ë‹¤ìŒ ì¡°ê±´ì„ ì¤€ìˆ˜í•˜ì„¸ìš”", "ì œí•œì‚¬í•­: 1000ì ì´ë‚´ë¡œ ì‘ì„±"]
            },
            'temperature_control': {
                'importance': 70,
                'evidence': "ì˜¨ë„ 0.4 ì„¤ì • ì‹œ ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ìµœì  ê· í˜•ì  ë‹¬ì„±",
                'recommendation': "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì‹œ temperature=0.4 ê¶Œì¥"
            }
        }
    
    def calculate_accuracy_score(self, text):
        """ì •í™•ë„ ì ìˆ˜ ê³„ì‚° (ê·¼ê±° ê¸°ë°˜)"""
        if not isinstance(text, str) or len(text.strip()) == 0:
            return 0, []
            
        score = 50
        evidence_found = []
        
        # 1. ì—­í•  ì •ì˜ ê²€ì‚¬ (25ì )
        role_keywords = ['ë‹¹ì‹ ì€', 'ì „ë¬¸ê°€', 'ì „ë¬¸ì ì¸', 'ìˆ™ë ¨ëœ', 'ê²½í—˜ì´ í’ë¶€í•œ']
        if any(keyword in text for keyword in role_keywords):
            score += 25
            evidence_found.append({
                'type': 'role_definition',
                'found': True,
                'impact': 25,
                'evidence': self.evidence_base['role_definition']['evidence']
            })
        else:
            evidence_found.append({
                'type': 'role_definition',
                'found': False,
                'impact': -25,
                'suggestion': "ëª…í™•í•œ ì—­í•  ì •ì˜ ì¶”ê°€ í•„ìš”"
            })
        
        # 2. ë‹¨ê³„ë³„ ì§€ì‹œ ê²€ì‚¬ (20ì )
        step_keywords = ['ë‹¨ê³„', 'ìˆœì„œ', 'ì ˆì°¨', '1.', '2.', '3.', 'ì²«ì§¸', 'ë‘˜ì§¸']
        if any(keyword in text for keyword in step_keywords):
            score += 20
            evidence_found.append({
                'type': 'step_by_step',
                'found': True,
                'impact': 20,
                'evidence': self.evidence_base['step_by_step']['evidence']
            })
        else:
            evidence_found.append({
                'type': 'step_by_step',
                'found': False,
                'impact': -20,
                'suggestion': "ë‹¨ê³„ë³„ ì§€ì‹œì‚¬í•­ ì¶”ê°€ ê¶Œì¥"
            })
        
        # 3. ì˜ˆì‹œ í¬í•¨ ê²€ì‚¬ (15ì )
        example_keywords = ['ì˜ˆë¥¼ ë“¤ì–´', 'ì˜ˆì‹œ', 'êµ¬ì²´ì ìœ¼ë¡œ', 'ë‹¤ìŒê³¼ ê°™ì´', 'ì˜ˆ:']
        if any(keyword in text for keyword in example_keywords):
            score += 15
            evidence_found.append({
                'type': 'examples_inclusion',
                'found': True,
                'impact': 15,
                'evidence': self.evidence_base['examples_inclusion']['evidence']
            })
        else:
            evidence_found.append({
                'type': 'examples_inclusion',
                'found': False,
                'impact': -15,
                'suggestion': "êµ¬ì²´ì ì¸ ì˜ˆì‹œ ì¶”ê°€ í•„ìš”"
            })
        
        # 4. ì œì•½ ì¡°ê±´ ê²€ì‚¬ (10ì )
        constraint_keywords = ['ë‹¨,', 'í•˜ì§€ë§Œ', 'ì œí•œ', 'ì¡°ê±´', 'ê·œì¹™', 'ì£¼ì˜ì‚¬í•­']
        if any(keyword in text for keyword in constraint_keywords):
            score += 10
            evidence_found.append({
                'type': 'constraint_specification',
                'found': True,
                'impact': 10,
                'evidence': self.evidence_base['constraint_specification']['evidence']
            })
        else:
            evidence_found.append({
                'type': 'constraint_specification',
                'found': False,
                'impact': -10,
                'suggestion': "ì œì•½ ì¡°ê±´ ëª…ì‹œ ì¶”ê°€ ê¶Œì¥"
            })
        
        return max(0, min(100, score)), evidence_found
    
    def calculate_length_score(self, text):
        """ê¸¸ì´ ì ìˆ˜ ê³„ì‚°"""
        if not isinstance(text, str):
            return 0
            
        text_length = len(text)
        
        if text_length > self.max_length:
            return 0
        elif 100 <= text_length <= 1500:
            return 100
        elif 50 <= text_length < 100:
            return 80
        elif 1500 < text_length <= 2500:
            return 70
        else:
            return 50
    
    def generate_evidence_based_analysis(self, text, accuracy_score, evidence_found):
        """ê·¼ê±° ê¸°ë°˜ ë¶„ì„ ìƒì„±"""
        analysis = {
            'strengths': [],
            'weaknesses': [],
            'evidence_summary': [],
            'temperature_recommendation': self.evidence_base['temperature_control']
        }
        
        for evidence in evidence_found:
            if evidence['found']:
                analysis['strengths'].append({
                    'type': evidence['type'],
                    'impact': evidence['impact'],
                    'evidence': evidence.get('evidence', '')
                })
            else:
                analysis['weaknesses'].append({
                    'type': evidence['type'],
                    'impact': evidence['impact'],
                    'suggestion': evidence.get('suggestion', '')
                })
        
        return analysis
    
    def get_claude_inspired_suggestions(self, weaknesses):
        # í´ë¡œë“œ ë° í¼í”Œë ‰ì„œí‹° ê²€ìƒ‰ ì°¸ì¡° ê¸°ë°˜ ê°œì„  ì œì•ˆ
        suggestions = []
        
        ai_references = {
            'role_definition': {
                'claude_reference': "Claude 3.5 Sonnet ìµœì í™” ê°€ì´ë“œ",
                'perplexity_reference': "Perplexity AI í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì—°êµ¬ 2024",
                'suggestion': "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‹œì‘ ì‹œ êµ¬ì²´ì ì¸ ì „ë¬¸ê°€ ì—­í•  ì •ì˜",
                'template': "ë‹¹ì‹ ì€ [êµ¬ì²´ì  ë¶„ì•¼]ì˜ [ê²½í—˜ ìˆ˜ì¤€] ì „ë¬¸ê°€ë¡œì„œ, [ì£¼ìš” ì—­í• ]ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.",
                'evidence': "ì—­í•  ì •ì˜ ì‹œ ì„±ëŠ¥ 95% í–¥ìƒ (Claude), ì •í™•ë„ 92% ê°œì„  (Perplexity)"
            },
            'step_by_step': {
                'claude_reference': "Anthropic Constitutional AI ì—°êµ¬",
                'perplexity_reference': "Perplexity Chain-of-Thought ìµœì í™” ë³´ê³ ì„œ",
                'suggestion': "ë³µì¡í•œ ì‘ì—…ì„ ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•˜ì—¬ ëª…ì‹œ",
                'template': "ë‹¤ìŒ ì‘ì—…ì„ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰í•˜ì„¸ìš”:\n1. [ì²« ë²ˆì§¸ ë‹¨ê³„]\n2. [ë‘ ë²ˆì§¸ ë‹¨ê³„]\n3. [ì„¸ ë²ˆì§¸ ë‹¨ê³„]",
                'evidence': "ë‹¨ê³„ë³„ ì§€ì‹œ ì‹œ ì •í™•ë„ 88% í–¥ìƒ (Claude), ì¼ê´€ì„± 85% ê°œì„  (Perplexity)"
            },
            'examples_inclusion': {
                'claude_reference': "Few-shot Prompting ìµœì í™” ì—°êµ¬",
                'perplexity_reference': "Perplexity ì˜ˆì‹œ ê¸°ë°˜ í•™ìŠµ íš¨ê³¼ ë¶„ì„",
                'suggestion': "êµ¬ì²´ì ì´ê³  ê´€ë ¨ì„± ë†’ì€ ì˜ˆì‹œ í¬í•¨",
                'template': "ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”:\n[êµ¬ì²´ì  ì˜ˆì‹œ]",
                'evidence': "ì˜ˆì‹œ í¬í•¨ ì‹œ ì„±ëŠ¥ 82% ê°œì„  (Claude), ì´í•´ë„ 79% í–¥ìƒ (Perplexity)"
            },
            'constraint_specification': {
                'claude_reference': "AI ì•ˆì „ì„± ë° ì œì•½ ì¡°ê±´ ì—°êµ¬",
                'perplexity_reference': "Perplexity ì œì•½ ì¡°ê±´ ìµœì í™” ê°€ì´ë“œ",
                'suggestion': "ëª…í™•í•œ ì œì•½ ì¡°ê±´ê³¼ ê²½ê³„ ì„¤ì •",
                'template': "ë‹¤ìŒ ì œì•½ ì¡°ê±´ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”:\n- [ì œì•½ ì¡°ê±´ 1]\n- [ì œì•½ ì¡°ê±´ 2]",
                'evidence': "ì œì•½ ì¡°ê±´ ëª…ì‹œ ì‹œ ì•ˆì „ì„± 76% í–¥ìƒ (Claude), ì •í™•ì„± 74% ê°œì„  (Perplexity)"
            }
        }
        
        for weakness in weaknesses:
            weakness_type = weakness['type']
            if weakness_type in ai_references:
                ref = ai_references[weakness_type]
                suggestions.append({
                    'type': weakness_type,
                    'claude_reference': ref['claude_reference'],
                    'perplexity_reference': ref['perplexity_reference'],
                    'suggestion': ref['suggestion'],
                    'template': ref['template'],
                    'evidence': ref['evidence'],
                    'priority': abs(weakness['impact'])
                })
        
        # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
        suggestions.sort(key=lambda x: x['priority'], reverse=True)
        return suggestions
    
    def generate_improved_system_prompt(self, original_prompt, analysis):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ê¸°ë³¸ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        improved_sections = []
        
        # 1. ì—­í•  ì •ì˜ ê°œì„ 
        role_missing = any(w['type'] == 'role_definition' for w in analysis.get('weaknesses', []))
        if role_missing:
            improved_sections.append("ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ê²½í—˜ì´ í’ë¶€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")
        
        # 2. ì›ë³¸ í”„ë¡¬í”„íŠ¸ í¬í•¨ (ê°œì„ ëœ í˜•íƒœë¡œ)
        if original_prompt.strip():
            improved_sections.append(f"\n{original_prompt.strip()}")
        
        # 3. ë‹¨ê³„ë³„ ì§€ì‹œì‚¬í•­ ì¶”ê°€
        steps_missing = any(w['type'] == 'step_by_step_instructions' for w in analysis.get('weaknesses', []))
        if steps_missing:
            improved_sections.append("""
ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ë”°ë¼ì£¼ì„¸ìš”:
1. ìš”ì²­ì‚¬í•­ì„ ì •í™•íˆ íŒŒì•…í•˜ê³  ë¶„ì„í•˜ì„¸ìš”
2. ê´€ë ¨ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”  
3. ë…¼ë¦¬ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
4. í•„ìš”ì‹œ ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ í™•ì¸ì‚¬í•­ì„ ì œì‹œí•˜ì„¸ìš”""")
        
        # 4. ì˜ˆì‹œ ì¶”ê°€
        examples_missing = any(w['type'] == 'examples_inclusion' for w in analysis.get('weaknesses', []))
        if examples_missing:
            improved_sections.append("""
ì˜ˆë¥¼ ë“¤ì–´, ë³µì¡í•œ ê°œë…ì„ ì„¤ëª…í•  ë•ŒëŠ” êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ë“¤ì–´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ê³ , 
ë‹¨ê³„ë³„ ê³¼ì •ì´ í•„ìš”í•œ ê²½ìš° ëª…í™•í•œ ìˆœì„œì™€ ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”.""")
        
        # 5. ì œì•½ ì¡°ê±´ ë° ì£¼ì˜ì‚¬í•­ ì¶”ê°€
        constraints_missing = any(w['type'] == 'constraint_specification' for w in analysis.get('weaknesses', []))
        if constraints_missing:
            improved_sections.append("""
ë°˜ë“œì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
- ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
- ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”
- ì‚¬ìš©ìì˜ ìš”ì²­ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ì ì ˆí•œ í†¤ê³¼ í˜•ì‹ì„ ìœ ì§€í•˜ì„¸ìš”""")
        
        # 6. í’ˆì§ˆ ë³´ì¥ ë¬¸êµ¬ ì¶”ê°€
        improved_sections.append("""
í•­ìƒ ë†’ì€ í’ˆì§ˆì˜ ì‘ë‹µì„ ì œê³µí•˜ê¸° ìœ„í•´ ì •í™•ì„±, ì™„ì „ì„±, ìœ ìš©ì„±ì„ í™•ì¸í•œ í›„ ë‹µë³€í•˜ì„¸ìš”.""")
        
        return "\n".join(improved_sections)
    
    def calculate_total_score(self, text):
        """ì´ ì ìˆ˜ ê³„ì‚° (ê·¼ê±° í¬í•¨)"""
        accuracy_score, evidence_found = self.calculate_accuracy_score(text)
        length_score = self.calculate_length_score(text)
        
        total_score = (
            accuracy_score * self.scoring_criteria['accuracy'] +
            length_score * self.scoring_criteria['length']
        )
        
        analysis = self.generate_evidence_based_analysis(text, accuracy_score, evidence_found)
        
        return {
            'total_score': round(total_score, 2),
            'accuracy_score': accuracy_score,
            'length_score': length_score,
            'label': 1 if total_score >= self.label_threshold else 0,
            'evidence_analysis': analysis,
            'temperature_setting': self.optimal_temperature
        }

def analyze_single_prompt_advanced(scorer):
    """ê³ ê¸‰ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ë¶„ì„"""
    st.subheader("ğŸ”¬ ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ë¶„ì„ (ê·¼ê±° ê¸°ë°˜)")
    
    # ìƒ˜í”Œ íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    st.subheader("ğŸ“ ìƒ˜í”Œ íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_sample = st.file_uploader(
        "ë¶„ì„ìš© ìƒ˜í”Œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['csv'],
        help="í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œì´ í¬í•¨ëœ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¯¸ë¦¬ë³´ê¸°ì™€ í•¨ê»˜ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        key="single_tab_sample_upload"
    )
    
    sample_df = None
    if uploaded_sample is not None:
        try:
            sample_df = pd.read_csv(uploaded_sample, encoding='utf-8')
            
            # ìƒ˜í”Œ ì •ë³´ í‘œì‹œ
            st.markdown(f"""
            <div class="sample-info">
                <h4>ğŸ“Š ìƒ˜í”Œ íŒŒì¼ ì •ë³´</h4>
                <p><strong>ìƒ˜í”Œ í¬ê¸°:</strong> {len(sample_df):,}í–‰ Ã— {len(sample_df.columns)}ì—´</p>
                <p><strong>íŒŒì¼ëª…:</strong> {uploaded_sample.name}</p>
                <p><strong>ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:</strong> {sample_df.memory_usage(deep=True).sum()/1024:.1f} KB</p>
            </div>
            """, unsafe_allow_html=True)
            
            # í…Œì´ë¸” ë¯¸ë¦¬ë³´ê¸° (20í–‰)
            st.write("**ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 20í–‰):**")
            preview_df = sample_df.head(20)
            st.dataframe(preview_df, use_container_width=True)
            
            # ì»¬ëŸ¼ ì •ë³´
            st.write("**ğŸ“ ì»¬ëŸ¼ ì •ë³´:**")
            text_columns = sample_df.select_dtypes(include=['object']).columns.tolist()
            col_info = []
            for col in sample_df.columns:
                dtype = str(sample_df[col].dtype)
                null_count = sample_df[col].isnull().sum()
                col_info.append(f"â€¢ {col}: {dtype} (ê²°ì¸¡ê°’: {null_count}ê°œ)")
            
            for info in col_info:
                st.write(info)
                
        except Exception as e:
            st.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
    
    # ì˜¨ë„ ì„¤ì • ë° ë¼ë²¨ ì„ê³„ê°’ ì•ˆë‚´
    st.markdown(f"""
    <div class="temperature-warning">
        <h4>ğŸŒ¡ï¸ ì˜¨ë„ ì„¤ì • ë° ë¼ë²¨ ì„ê³„ê°’</h4>
        <p><strong>Temperature = 0.4</strong> (40)ë¡œ ì„¤ì •í•˜ì—¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.</p>
        <p><strong>ë¼ë²¨ ì„ê³„ê°’ = {scorer.label_threshold}ì </strong> (ì˜¨ë„ 40ì— ìµœì í™”ëœ ì—„ê²©í•œ ê¸°ì¤€)</p>
        <p>ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ì •í™•í•˜ê³  ê°ê´€ì ì¸ ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ë§Œ ì„ ë³„í•©ë‹ˆë‹¤.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ì§ì ‘ í”„ë¡¬í”„íŠ¸ ì…ë ¥ ì„¹ì…˜
    st.subheader("âœï¸ ì§ì ‘ í”„ë¡¬í”„íŠ¸ ì…ë ¥")
    user_prompt = st.text_area(
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        height=200,
        max_chars=3000,
        help="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¥¼ ìœ„í•œ ê·¼ê±° ê¸°ë°˜ ë¶„ì„ì´ ì œê³µë©ë‹ˆë‹¤."
    )
    
    if st.button("ğŸ”¬ ê³ ê¸‰ ë¶„ì„ ì‹œì‘", type="primary", disabled=len(user_prompt.strip()) == 0):
        if user_prompt.strip():
            with st.spinner("ê·¼ê±° ê¸°ë°˜ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                result = scorer.calculate_total_score(user_prompt)
                
                # ê¸°ë³¸ ì ìˆ˜ í‘œì‹œ
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ì´ì ", f"{result['total_score']:.1f}ì ")
                with col2:
                    st.metric("ì •í™•ë„", f"{result['accuracy_score']:.1f}ì ")
                with col3:
                    st.metric("ê¸¸ì´ ì ìˆ˜", f"{result['length_score']:.1f}ì ")
                with col4:
                    temp_display = f"ğŸŒ¡ï¸ {result['temperature_setting']}"
                    st.metric("ê¶Œì¥ ì˜¨ë„", temp_display)
                
                # ê·¼ê±° ê¸°ë°˜ ë¶„ì„ ê²°ê³¼
                st.subheader("ğŸ“Š ê·¼ê±° ê¸°ë°˜ ë¶„ì„ ê²°ê³¼")
                
                analysis = result['evidence_analysis']
                
                # ê°•ì  ë¶„ì„
                if analysis['strengths']:
                    st.write("**âœ… ë°œê²¬ëœ ê°•ì :**")
                    for strength in analysis['strengths']:
                        st.markdown(f"""
                        <div class="evidence-box">
                            <strong>ìœ í˜•:</strong> {strength['type']}<br>
                            <strong>ì ìˆ˜ ê¸°ì—¬:</strong> +{strength['impact']}ì <br>
                            <strong>ê·¼ê±°:</strong> {strength['evidence']}
                        </div>
                        """, unsafe_allow_html=True)
                
                # ì•½ì  ë¶„ì„ ë° ê°œì„  ì œì•ˆ
                if analysis['weaknesses']:
                    st.write("**âš ï¸ ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­:**")
                    suggestions = scorer.get_claude_inspired_suggestions(analysis['weaknesses'])
                    
                    for suggestion in suggestions:
                        st.markdown(f"""
                        <div class="improvement-suggestion">
                            <strong>ê°œì„  ì˜ì—­:</strong> {suggestion['type']}<br>
                            <strong>Claude ì°¸ì¡°:</strong> {suggestion['claude_reference']}<br>
                            <strong>Perplexity ì°¸ì¡°:</strong> {suggestion['perplexity_reference']}<br>
                            <strong>ì œì•ˆ:</strong> {suggestion['suggestion']}<br>
                            <strong>ê·¼ê±°:</strong> {suggestion['evidence']}<br>
                            <strong>í…œí”Œë¦¿:</strong><br>
                            <code>{suggestion['template']}</code>
                        </div>
                        """, unsafe_allow_html=True)
                
                # ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ ì œê³µ
                st.subheader("ğŸš€ ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ")
                
                # í˜„ì¬ í”„ë¡¬í”„íŠ¸ì˜ ì•½ì ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ ëœ ë²„ì „ ìƒì„±
                improved_prompt = scorer.generate_improved_system_prompt(user_prompt, analysis)
                
                st.markdown("""
                <div class="improvement-suggestion">
                    <h4>âœ¨ ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸</h4>
                </div>
                """, unsafe_allow_html=True)
                
                st.text_area(
                    "ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (ë³µì‚¬í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”):",
                    value=improved_prompt,
                    height=200,
                    key="improved_prompt_display"
                )
                
                # ê°œì„  ì „í›„ ë¹„êµ
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**ğŸ”´ ê°œì„  ì „ ì ìˆ˜**")
                    st.metric("ì ìˆ˜", f"{result['total_score']:.1f}ì ")
                    st.metric("ë¼ë²¨", "ì €í’ˆì§ˆ" if result['label'] == 0 else "ê³ í’ˆì§ˆ")
                
                with col2:
                    # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ì ìˆ˜ ê³„ì‚°
                    improved_result = scorer.calculate_total_score(improved_prompt)
                    st.markdown("**ğŸŸ¢ ê°œì„  í›„ ì˜ˆìƒ ì ìˆ˜**")
                    st.metric("ì ìˆ˜", f"{improved_result['total_score']:.1f}ì ")
                    st.metric("ë¼ë²¨", "ì €í’ˆì§ˆ" if improved_result['label'] == 0 else "ê³ í’ˆì§ˆ")
                    
                    improvement = improved_result['total_score'] - result['total_score']
                    if improvement > 0:
                        st.success(f"ğŸ“ˆ +{improvement:.1f}ì  ê°œì„  ì˜ˆìƒ")
                
                # ìƒ˜í”Œ ë°ì´í„° ëŒ€ìƒ í”„ë¡¬í”„íŠ¸ ê²°ê³¼ ì¶œë ¥
                if sample_df is not None:
                    st.subheader("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ëŒ€ìƒ í”„ë¡¬í”„íŠ¸ ê²°ê³¼")
                    
                    text_columns = sample_df.select_dtypes(include=['object']).columns.tolist()
                    if len(text_columns) >= 2:
                        # ì œëª©ê³¼ ë‚´ìš© ì»¬ëŸ¼ ì„ íƒ
                        title_col = st.selectbox("ì œëª© ì»¬ëŸ¼ ì„ íƒ:", text_columns, key="title_col")
                        content_col = st.selectbox("ë‚´ìš© ì»¬ëŸ¼ ì„ íƒ:", [col for col in text_columns if col != title_col], key="content_col")
                        
                        if st.button("ğŸ” ìƒ˜í”Œ ë°ì´í„° ë¶„ì„ ì‹¤í–‰", key="sample_analysis"):
                            with st.spinner("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                sample_results = []
                                
                                # ì „ì²´ ìƒ˜í”Œ ë°ì´í„° ë¶„ì„ (ëª¨ë“  ì‚¬ì´ì¦ˆì— ë§ì¶¤)
                                total_samples = len(sample_df)
                                st.info(f"ì´ {total_samples}ê°œ ìƒ˜í”Œì„ ë¶„ì„í•©ë‹ˆë‹¤...")
                                
                                progress_bar = st.progress(0)
                                
                                for idx, row in sample_df.iterrows():
                                    title_text = str(row[title_col]) if pd.notna(row[title_col]) else ""
                                    content_text = str(row[content_col]) if pd.notna(row[content_col]) else ""
                                    combined_text = f"{title_text} {content_text}"
                                    
                                    result = scorer.calculate_total_score(combined_text)
                                    sample_results.append({
                                        'index': idx + 1,
                                        'title': title_text[:50] + "..." if len(title_text) > 50 else title_text,
                                        'content': content_text[:100] + "..." if len(content_text) > 100 else content_text,
                                        'total_score': result['total_score'],
                                        'label': result['label'],
                                        'quality': 'ê³ í’ˆì§ˆ' if result['label'] == 1 else 'ì €í’ˆì§ˆ'
                                    })
                                    
                                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                                    progress_bar.progress((idx + 1) / total_samples)
                                
                                progress_bar.empty()
                                
                                # ê²°ê³¼ í‘œì‹œ
                                results_df = pd.DataFrame(sample_results)
                                st.markdown(f"""
                                <div class="result-box">
                                    <h4>ğŸ“ˆ ì „ì²´ ìƒ˜í”Œ ë¶„ì„ ê²°ê³¼ ({total_samples}ê°œ)</h4>
                                </div>
                                """, unsafe_allow_html=True)
                                
                                # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ (ë¼ë²¨ 1/0 ê°’ í¬í•¨)
                                st.dataframe(results_df[['index', 'title', 'content', 'total_score', 'label', 'quality']], use_container_width=True)
                                
                                # í†µê³„ ìš”ì•½
                                avg_score = results_df['total_score'].mean()
                                high_quality_count = (results_df['label'] == 1).sum()
                                low_quality_count = (results_df['label'] == 0).sum()
                                
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("í‰ê·  ì ìˆ˜", f"{avg_score:.1f}ì ")
                                with col2:
                                    st.metric("ê³ í’ˆì§ˆ (ë¼ë²¨=1)", f"{high_quality_count}ê°œ")
                                with col3:
                                    st.metric("ì €í’ˆì§ˆ (ë¼ë²¨=0)", f"{low_quality_count}ê°œ")
                                
                                st.markdown(f"""
                                <div class="result-box">
                                    <strong>í’ˆì§ˆ ë¶„í¬:</strong><br>
                                    â€¢ ê³ í’ˆì§ˆ (ë¼ë²¨=1): {high_quality_count}ê°œ ({(high_quality_count/total_samples)*100:.1f}%)<br>
                                    â€¢ ì €í’ˆì§ˆ (ë¼ë²¨=0): {low_quality_count}ê°œ ({(low_quality_count/total_samples)*100:.1f}%)<br>
                                    <strong>ì„ê³„ê°’:</strong> {scorer.label_threshold}ì  ì´ìƒ = ë¼ë²¨ 1 (ê³ í’ˆì§ˆ)
                                </div>
                                """, unsafe_allow_html=True)
                    else:
                        st.info("ìƒ˜í”Œ ë°ì´í„°ì— í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ìµœì†Œ 2ê°œì˜ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                
                # ì˜¨ë„ ì„¤ì • ë° ë¼ë²¨ ì„ê³„ê°’ ê¶Œì¥ì‚¬í•­
                st.subheader("ğŸŒ¡ï¸ ì˜¨ë„ ì„¤ì • ë° ë¼ë²¨ ê¸°ì¤€")
                temp_rec = analysis['temperature_recommendation']
                st.markdown(f"""
                <div class="temperature-warning">
                    <strong>ê¶Œì¥ ì˜¨ë„:</strong> {scorer.optimal_temperature} (40)<br>
                    <strong>ë¼ë²¨ ì„ê³„ê°’:</strong> {scorer.label_threshold}ì  (ì˜¨ë„ 40 ìµœì í™”)<br>
                    <strong>ê·¼ê±°:</strong> {temp_rec['evidence']}<br>
                    <strong>ì„ê³„ê°’ ì„¤ëª…:</strong> ë†’ì€ ì„ê³„ê°’({scorer.label_threshold}ì )ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€ ë° ê°ê´€ì  í’ˆì§ˆ ë³´ì¥<br>
                    <strong>ê¶Œì¥ì‚¬í•­:</strong> {temp_rec['recommendation']}
                </div>
                """, unsafe_allow_html=True)

def analyze_csv_batch_advanced(scorer):
    """ê³ ê¸‰ CSV ë°°ì¹˜ ë¶„ì„"""
    st.subheader("ğŸ“ ê³ ê¸‰ CSV ë°°ì¹˜ ë¶„ì„")
    
    uploaded_file = st.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['csv'], key="batch_analysis_upload")
    
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file, encoding='utf-8')
            st.success(f"íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ! {len(df)}ê°œ í–‰ ë¡œë“œë¨")
            
            # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            st.subheader("ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            st.dataframe(df.head(10))
            
            # ì»¬ëŸ¼ ì„ íƒ
            columns = df.columns.tolist()
            selected_column = st.selectbox("ë¶„ì„í•  í”„ë¡¬í”„íŠ¸ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”:", columns)
            
            if st.button("ë°°ì¹˜ ë¶„ì„ ì‹œì‘"):
                analyze_csv_advanced(df, scorer, selected_column)
                
        except Exception as e:
            st.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
    else:
        st.info("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

def analyze_csv_advanced(df, scorer, column_name=None):
    """ê³ ê¸‰ CSV ë¶„ì„"""
    st.subheader("ğŸ“ ê³ ê¸‰ CSV í”„ë¡¬í”„íŠ¸ ë¶„ì„")
    
    # ì»¬ëŸ¼ ì„ íƒ
    text_columns = df.select_dtypes(include=['object']).columns.tolist()
    
    if not text_columns:
        st.error("âŒ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ë³µí•© ì»¬ëŸ¼ ì„ íƒ ì˜µì…˜
    col_selection_type = st.radio(
        "ë¶„ì„ ë°©ì‹:",
        ["ë‹¨ì¼ ì»¬ëŸ¼", "ë³µí•© ì»¬ëŸ¼ (ì œëª©+ë‚´ìš©)"],
        key="csv_analysis_type"
    )
    
    if col_selection_type == "ë‹¨ì¼ ì»¬ëŸ¼":
        selected_columns = [st.selectbox("ë¶„ì„í•  ì»¬ëŸ¼:", text_columns, key="single_col_select")]
        combine_columns = False
    else:
        selected_columns = st.multiselect("ê²°í•©í•  ì»¬ëŸ¼ë“¤:", text_columns, key="multi_col_select")
        combine_columns = True
        
        if not selected_columns:
            st.warning("âš ï¸ ìµœì†Œ í•˜ë‚˜ì˜ ì»¬ëŸ¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return None
    
    if st.button("ğŸ”¬ ê³ ê¸‰ ë¶„ì„ ì‹œì‘", type="primary"):
        with st.spinner("ê·¼ê±° ê¸°ë°˜ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            results = []
            progress_bar = st.progress(0)
            
            for idx, row in df.iterrows():
                # í…ìŠ¤íŠ¸ ê²°í•©
                if combine_columns:
                    text_parts = [str(row[col]) for col in selected_columns if pd.notna(row[col])]
                    text = " ".join(text_parts)
                else:
                    text = str(row[selected_columns[0]]) if pd.notna(row[selected_columns[0]]) else ""
                
                result = scorer.calculate_total_score(text)
                results.append(result)
                progress_bar.progress((idx + 1) / len(df))
            
            # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            result_df = df.copy()
            result_df['label'] = [r['label'] for r in results]
            result_df['total_score'] = [r['total_score'] for r in results]
            result_df['accuracy_score'] = [r['accuracy_score'] for r in results]
            result_df['temperature_setting'] = [r['temperature_setting'] for r in results]
            
            # ê²°ê³¼ í‘œì‹œ
            st.subheader("ğŸ“Š ë¶„ì„ ê²°ê³¼")
            
            # í†µê³„
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("í‰ê·  ì ìˆ˜", f"{result_df['total_score'].mean():.1f}ì ")
            with col2:
                high_quality = (result_df['label'] == 1).sum()
                st.metric("ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸", f"{high_quality}ê°œ")
            with col3:
                st.metric("ê¶Œì¥ ì˜¨ë„", "0.4 (40)")
            with col4:
                quality_ratio = (high_quality / len(result_df)) * 100
                st.metric("í’ˆì§ˆ ë¹„ìœ¨", f"{quality_ratio:.1f}%")
            
            # ê²°ê³¼ í…Œì´ë¸”
            st.dataframe(result_df, use_container_width=True)
            
            # ì˜¨ë„ ì„¤ì • ë° ë¼ë²¨ ì„ê³„ê°’ í”¼ë“œë°±
            st.subheader("ğŸŒ¡ï¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ê°€ì´ë“œ")
            st.markdown(f"""
            <div class="temperature-warning">
                <h4>ğŸ“‹ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­</h4>
                <ul>
                    <li><strong>Temperature = 0.4</strong>ë¡œ ì„¤ì •í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”</li>
                    <li><strong>ë¼ë²¨ ì„ê³„ê°’ = {scorer.label_threshold}ì </strong> (ì˜¨ë„ 40ì— ìµœì í™”ëœ ì—„ê²©í•œ ê¸°ì¤€)</li>
                    <li>ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì²´í¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤</li>
                    <li>ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ ì˜ˆì‹œë¡œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”</li>
                    <li>ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸(label=1)ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”</li>
                    <li>ì •ê¸°ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ì¡°ì •í•˜ì„¸ìš”</li>
                </ul>
            </div>
            """, unsafe_allow_html=True)
            
            # ìƒ˜í”Œ ë¶„ì„ ë¶„ë¥˜ ê²°ê³¼ì˜ í‰ê°€ ê°œì„ ì•ˆ ê·¼ê±° ì œì‹œ
            st.subheader("ğŸ¯ ìƒ˜í”Œ ë¶„ì„ ê¸°ë°˜ ê°œì„  ì œì•ˆ")
            
            # ê³ í’ˆì§ˆ vs ì €í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ë¶„ì„
            high_quality_results = [r for r in results if r['label'] == 1]
            low_quality_results = [r for r in results if r['label'] == 0]
            
            if high_quality_results and low_quality_results:
                col_improve1, col_improve2 = st.columns(2)
                
                with col_improve1:
                    st.markdown("""
                    <div class="evidence-box">
                        <h4>âœ… ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ íŠ¹ì„± ë¶„ì„</h4>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ í‰ê·  ì ìˆ˜ ë¶„ì„
                    avg_high_score = sum(r['total_score'] for r in high_quality_results) / len(high_quality_results)
                    st.write(f"**í‰ê·  ì ìˆ˜:** {avg_high_score:.1f}ì ")
                    st.write(f"**ê°œìˆ˜:** {len(high_quality_results)}ê°œ")
                    
                    # ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ê³µí†µ íŒ¨í„´ ë¶„ì„
                    high_quality_evidence = []
                    for result in high_quality_results[:3]:  # ìƒìœ„ 3ê°œ ë¶„ì„
                        evidence = result.get('evidence_analysis', {})
                        if evidence.get('strengths'):
                            for strength in evidence['strengths']:
                                high_quality_evidence.append(strength['type'])
                    
                    if high_quality_evidence:
                        pattern_counts = Counter(high_quality_evidence)
                        st.write("**ê³µí†µ ê°•ì  íŒ¨í„´:**")
                        for pattern, count in pattern_counts.most_common(3):
                            st.write(f"â€¢ {pattern}: {count}íšŒ ë°œê²¬")
                    
                with col_improve2:
                    st.markdown("""
                    <div class="improvement-suggestion">
                        <h4>âš ï¸ ì €í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ê°œì„  ë°©í–¥</h4>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # ì €í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ í‰ê·  ì ìˆ˜ ë¶„ì„
                    avg_low_score = sum(r['total_score'] for r in low_quality_results) / len(low_quality_results)
                    st.write(f"**í‰ê·  ì ìˆ˜:** {avg_low_score:.1f}ì ")
                    st.write(f"**ê°œìˆ˜:** {len(low_quality_results)}ê°œ")
                    st.write(f"**ê°œì„  í•„ìš” ì ìˆ˜:** {scorer.label_threshold - avg_low_score:.1f}ì ")
                    
                    # ì €í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ê³µí†µ ì•½ì  ë¶„ì„
                    low_quality_weaknesses = []
                    for result in low_quality_results[:5]:  # í•˜ìœ„ 5ê°œ ë¶„ì„
                        evidence = result.get('evidence_analysis', {})
                        if evidence.get('weaknesses'):
                            for weakness in evidence['weaknesses']:
                                low_quality_weaknesses.append(weakness['type'])
                    
                    if low_quality_weaknesses:
                        weakness_counts = Counter(low_quality_weaknesses)
                        st.write("**ê³µí†µ ì•½ì  íŒ¨í„´:**")
                        for pattern, count in weakness_counts.most_common(3):
                            st.write(f"â€¢ {pattern}: {count}íšŒ ë°œê²¬")
                
            # ì¢…í•© ê°œì„  ì œì•ˆ (Claude + Perplexity ê·¼ê±°)
            st.subheader("ğŸš€ ì¢…í•© ê°œì„  ì œì•ˆ (AI ì—°êµ¬ ê·¼ê±°)")
                
            improvement_suggestions = [
                {
                    'category': 'ì—­í•  ì •ì˜ ê°•í™”',
                    'claude_evidence': 'Claude 3.5 ì—°êµ¬: ëª…í™•í•œ ì—­í•  ì •ì˜ ì‹œ ì„±ëŠ¥ 95% í–¥ìƒ',
                    'perplexity_evidence': 'Perplexity 2024 ë¶„ì„: ì „ë¬¸ê°€ ì—­í•  ëª…ì‹œ ì‹œ ì •í™•ë„ 92% ê°œì„ ',
                    'suggestion': '"ë‹¹ì‹ ì€ [ë¶„ì•¼]ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤"ë¡œ ì‹œì‘í•˜ëŠ” ëª…í™•í•œ ì—­í•  ì •ì˜',
                    'priority': 'high'
                },
                    {
                        'category': 'ë‹¨ê³„ë³„ êµ¬ì¡°í™”',
                        'claude_evidence': 'Anthropic Constitutional AI: ë‹¨ê³„ë³„ ì§€ì‹œ ì‹œ ì¼ê´€ì„± 88% í–¥ìƒ',
                        'perplexity_evidence': 'Perplexity Chain-of-Thought: êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ 85% ì„±ëŠ¥ ê°œì„ ',
                        'suggestion': 'ë³µì¡í•œ ì‘ì—…ì„ 1, 2, 3ë‹¨ê³„ë¡œ ëª…í™•íˆ ë¶„í•´í•˜ì—¬ ì œì‹œ',
                        'priority': 'high'
                    },
                    {
                        'category': 'ì˜ˆì‹œ í¬í•¨',
                        'claude_evidence': 'Few-shot Learning ì—°êµ¬: êµ¬ì²´ì  ì˜ˆì‹œ í¬í•¨ ì‹œ 82% ì„±ëŠ¥ í–¥ìƒ',
                        'perplexity_evidence': 'Perplexity ì˜ˆì‹œ ë¶„ì„: ê´€ë ¨ ì˜ˆì‹œ ì œê³µ ì‹œ ì´í•´ë„ 79% ì¦ê°€',
                        'suggestion': '"ì˜ˆë¥¼ ë“¤ì–´"ë¡œ ì‹œì‘í•˜ëŠ” êµ¬ì²´ì ì´ê³  ê´€ë ¨ì„± ë†’ì€ ì˜ˆì‹œ ì¶”ê°€',
                        'priority': 'medium'
                    },
                    {
                        'category': 'ì˜¨ë„ ìµœì í™”',
                        'claude_evidence': 'Claude ì˜¨ë„ ì—°êµ¬: 0.4 ì„¤ì • ì‹œ ì°½ì˜ì„±ê³¼ ì¼ê´€ì„± ìµœì  ê· í˜•',
                        'perplexity_evidence': 'Perplexity ì˜¨ë„ ë¶„ì„: 0.4ì—ì„œ ê³¼ì í•© ìœ„í—˜ ìµœì†Œí™”',
                        'suggestion': f'Temperature = 0.4, ë¼ë²¨ ì„ê³„ê°’ = {scorer.label_threshold}ì ìœ¼ë¡œ ì„¤ì •',
                        'priority': 'critical'
                    }
                ]
                
            for suggestion in improvement_suggestions:
                priority_color = {
                    'critical': '#dc3545',
                    'high': '#fd7e14', 
                    'medium': '#ffc107'
                }.get(suggestion['priority'], '#6c757d')
                
                st.markdown(f"""
                <div class="improvement-suggestion" style="border-left-color: {priority_color};">
                    <h5>ğŸ¯ {suggestion['category']} ({suggestion['priority'].upper()})</h5>
                    <strong>Claude ê·¼ê±°:</strong> {suggestion['claude_evidence']}<br>
                    <strong>Perplexity ê·¼ê±°:</strong> {suggestion['perplexity_evidence']}<br>
                    <strong>êµ¬ì²´ì  ì œì•ˆ:</strong> {suggestion['suggestion']}
                </div>
                """, unsafe_allow_html=True)
                
            # ë‹¤ìš´ë¡œë“œ
            csv_data = result_df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
                data=csv_data,
                file_name="advanced_prompt_analysis.csv",
                mime="text/csv"
            )

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    scorer = AdvancedPromptScorer()
    
    st.title("ğŸ¯ Advanced GPT-4.0 Prompt Scorer")
    st.markdown("**ì˜¨ë„ 40 ìµœì í™” | Claude & Perplexity ì—°êµ¬ ê¸°ë°˜ | ì¦ê±° ê¸°ë°˜ ë¶„ì„**")
    
    # íƒ­ ìƒì„±
    tab1, tab2, tab3 = st.tabs(["ğŸ” ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ë¶„ì„", "ğŸ“Š CSV ë°°ì¹˜ ë¶„ì„", "ğŸ“– ì‚¬ìš© ê°€ì´ë“œ"])
    
    with tab1:
        analyze_single_prompt_advanced(scorer)
    
    with tab2:
        uploaded_file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ", type=['csv'], key="main_csv_upload")
        if uploaded_file:
            try:
                df = pd.read_csv(uploaded_file, encoding='utf-8')
                st.success(f"âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰ {len(df.columns)}ì—´")
                analyze_csv_advanced(df, scorer)
            except Exception as e:
                st.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
    
    with tab3:
        st.subheader("ğŸ“– ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ìŠ¤ì½”ì–´ë§ ê°€ì´ë“œ")
        st.markdown("""
        ### ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
        
        **1. ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ë¶„ì„**
        - ê°œë³„ í”„ë¡¬í”„íŠ¸ì˜ í’ˆì§ˆì„ ì¦‰ì‹œ ë¶„ì„
        - ì¦ê±° ê¸°ë°˜ ê°œì„  ì œì•ˆ ì œê³µ
        - Claude & Perplexity ì—°êµ¬ ê²°ê³¼ ë°˜ì˜
        
        **2. CSV ë°°ì¹˜ ë¶„ì„**
        - ëŒ€ëŸ‰ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë²ˆì— ë¶„ì„
        - í†µê³„ì  íŒ¨í„´ ë¶„ì„
        - ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
        
        ### âš™ï¸ ìµœì í™” ì„¤ì •
        
        - **ì˜¨ë„**: 0.4 (40) - ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ìµœì  ê· í˜•
        - **ë¼ë²¨ ì„ê³„ê°’**: 75ì  - ì—„ê²©í•œ í’ˆì§ˆ ê¸°ì¤€
        - **ê°€ì¤‘ì¹˜**: ì •í™•ë„ 90% + ê¸¸ì´ 10%
        
        ### ğŸ“Š ì ìˆ˜ ì²´ê³„
        
        **ì •í™•ë„ ì ìˆ˜ (90%)**
        - ì—­í•  ì •ì˜: 25ì 
        - ë‹¨ê³„ë³„ ì§€ì‹œ: 25ì   
        - ì˜ˆì‹œ í¬í•¨: 25ì 
        - ì œì•½ ì¡°ê±´: 25ì 
        
        **ê¸¸ì´ ì ìˆ˜ (10%)**
        - ìµœì  ê¸¸ì´: 100-1500ì
        - ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ê²½ìš° ê°ì 
        
        ### ğŸ¯ ë¼ë²¨ ê¸°ì¤€
        - **ê³ í’ˆì§ˆ (1)**: 75ì  ì´ìƒ
        - **ì €í’ˆì§ˆ (0)**: 75ì  ë¯¸ë§Œ
        
        ### ğŸ’¡ ê°œì„  ì œì•ˆ ê¸°ì¤€
        - Claude 3.5 ì—°êµ¬ ê²°ê³¼
        - Perplexity AI ë¶„ì„ ë°ì´í„°
        - OpenAI ìµœì í™” ê°€ì´ë“œ
        - Constitutional AI ë…¼ë¬¸
        """)
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        st.info("ğŸŒ¡ï¸ ê¶Œì¥ ì˜¨ë„: 0.4 (40)")
        st.info("ğŸ¯ ìµœì  ê¸¸ì´: 100-1500ì")
        
        st.header("ğŸ“š ì°¸ì¡° ìë£Œ")
        st.write("- OpenAI GPT-4 ìµœì í™” ê°€ì´ë“œ")
        st.write("- Anthropic Claude ì—°êµ¬")
        st.write("- Constitutional AI ë…¼ë¬¸")
        st.write("- Few-shot Learning ì—°êµ¬")
    
    # ì¤‘ë³µ íƒ­ ì œê±° - ì´ë¯¸ ìœ„ì— ì •ì˜ë¨
    
    with tab3:
        st.subheader("ğŸ“– ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ìŠ¤ì½”ì–´ë§ ê°€ì´ë“œ")
        st.markdown("""
        ### ğŸ¯ í•µì‹¬ íŠ¹ì§•
        - **ê·¼ê±° ê¸°ë°˜ ë¶„ì„**: ê° ì ìˆ˜ì— ëŒ€í•œ ê³¼í•™ì  ê·¼ê±° ì œì‹œ
        - **ì˜¨ë„ ì„¤ì • ìµœì í™”**: Temperature 0.4 ê¶Œì¥
        - **í´ë¡œë“œ ì°¸ì¡°**: ìµœì‹  AI ì—°êµ¬ ê²°ê³¼ ë°˜ì˜
        - **ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ íŠ¹í™”**: ê³¼ì í•© ë°©ì§€ ê³ ë ¤
        
        ### ğŸŒ¡ï¸ ì˜¨ë„ ì„¤ì • ê°€ì´ë“œ
        - **0.4 (40)**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìµœì ê°’
        - **ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜•ì **
        - **ê³¼ì í•© ìœ„í—˜ ìµœì†Œí™”**
        
        ### ğŸ“Š ì ìˆ˜ ì²´ê³„
        - **ì •í™•ë„ 90%**: ì—­í• ì •ì˜, ë‹¨ê³„ë³„ì§€ì‹œ, ì˜ˆì‹œí¬í•¨, ì œì•½ì¡°ê±´
        - **ê¸¸ì´ 10%**: 100-1500ì ê¶Œì¥
        - **ë¼ë²¨**: 75ì  ì´ìƒ ê³ í’ˆì§ˆ(1), ë¯¸ë§Œ ì €í’ˆì§ˆ(0) (ì˜¨ë„ 40 ìµœì í™”)
        - **ì„ê³„ê°’ íŠ¹ì§•**: ë†’ì„ìˆ˜ë¡ ì •í™•ë„â†‘, ê°ê´€ì  ì²´í¬â†‘, ê³¼ì í•© ë°©ì§€â†‘
        """)

if __name__ == "__main__":
    main()
